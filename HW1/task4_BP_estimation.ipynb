{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network for Regression (Estimate blood pressure from PPG signal)\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [HW page](http://kovan.ceng.metu.edu.tr/~sinan/DL/index.html) on the course website.*\n",
    "\n",
    "Having gained some experience with neural networks, let us train a network that estimates the blood pressure from a PPG signal window.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Photoplethysmograph (PPG) signal\n",
    "\n",
    "A PPG (photoplethysmograph) signal is a signal obtained with a pulse oximeter, which illuminates the skin and measures changes in light absorption. A PPG signal carries rich information about the status of the cardiovascular health of a person, such as breadth rate, heart rate and blood pressure. An example is shown below, where you also see the blood pressure signal that we will estimate (the data also has the ECG signal, which you should ignore).\n",
    "\n",
    "<img width=\"80%\" src=\"PPG_ABG_ECG_example.png\">\n",
    "\n",
    "\n",
    "# Constructing the Dataset \n",
    "\n",
    "In this task, you are expected to perform the full pipeline for creating a learning system from scratch. Here is how you should construct the dataset:\n",
    "* Download the dataset from the following website, and only take \"Part 1\" from it (it is too big): https://archive.ics.uci.edu/ml/datasets/Cuff-Less+Blood+Pressure+Estimation\n",
    "* Take a window of size $W$ from the PPG channel between time $t$ and $t+W$. Let us call this $\\textbf{x}_t$.\n",
    "* Take the corresponding window of size $W$ from the ABP (arterial blood pressure) channel between time $t$ and $t+W$. Find the maxima and minima of this signal within the window (you can use \"findpeaks\" from Matlab or \"find_peaks_cwt\" from scipy). Here is an example window from the ABP signal, and its peaks:\n",
    " <img width=\"60%\" src=\"ABP_peaks.png\">\n",
    "    \n",
    "* Calculate the average of the maxima, call it $y^1_t$, and the average of the minima, call it $y^2_t$.\n",
    "* Slide the window over the PPG signals and collect many $(\\textbf{x}_t, <y^1_t, y^2_t>)$ instances. In other words, your network outputs two values.\n",
    "* This will be your input-output for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from metu.data_utils import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "from cs231n.classifiers.neural_net_for_regression import TwoLayerNet\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "  np.random.seed(0)\n",
    "  return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "  np.random.seed(1)\n",
    "  X = 10 * np.random.randn(num_inputs, input_size)\n",
    "  y = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4], [2, 1, 4], [2, 1, 4]])\n",
    "  return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `cs231n/classifiers/neural_net_for_regression.py` and look at the method `TwoLayerNet.loss`. This function is very similar to the loss functions you have written for the previous exercises: It takes the data and weights and computes the *regression* scores, the squared error loss, and the gradients on the parameters. \n",
    "\n",
    "To be more specific, you will implement the following loss function:\n",
    "\n",
    "$$\\frac{1}{2}\\sum_i\\sum_{j} (o_{ij} - y_{ij})^2 + \\frac{1}{2}\\lambda\\sum_j w_j^2,$$\n",
    "\n",
    "where $i$ runs through the samples in the batch; $o_{ij}$ is the prediction of the network for the $i^{th}$ sample for output $j$, and $y_{ij}$ is the correct value; $\\lambda$ is the weight of the regularization term.\n",
    "\n",
    "The first layer uses ReLU as the activation function. The output layer does not use any activation functions.\n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "correct scores:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.6802720496109664e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X, dropout = 1.0)\n",
    "print 'Your scores:'\n",
    "print scores\n",
    "print\n",
    "print 'correct scores:'\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print correct_scores\n",
    "print\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print 'Difference between your scores and correct scores:'\n",
    "print np.sum(np.abs(scores - correct_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, implement the second part that computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "2.5480062504357193e-11\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1, dropout=1.0)\n",
    "correct_loss = 66.3406756909\n",
    "\n",
    "# should be very small, we get < 1e-10\n",
    "print 'Difference between your loss and correct loss:'\n",
    "print np.sum(np.abs(loss - correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Implement the rest of the function. This will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 5.463838e-04\n",
      "W2 max relative error: 3.755046e-04\n",
      "b2 max relative error: 1.443387e-06\n",
      "b1 max relative error: 2.188996e-07\n"
     ]
    }
   ],
   "source": [
    "from cs231n.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1, dropout=1.0)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "  f = lambda W: net.loss(X, y, reg=0.1, dropout=1.0)[0]\n",
    "  param_grad_num = eval_numerical_gradient(f, net.params[param_name])\n",
    "  print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PPG dataset for training your regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the training set:  23676\n",
      "Number of instances in the validation set:  2367\n",
      "Number of instances in the testing set:  3551\n"
     ]
    }
   ],
   "source": [
    "# Load the PPG dataset\n",
    "# If your memory turns out to be sufficient, try loading a subset\n",
    "import os\n",
    "def get_data(datafile, training_ratio=0.8, test_ratio=0.12, val_ratio=0.08):\n",
    "  # Load the PPG training data \n",
    "\n",
    "  X, y = load_dataset(datafile)\n",
    "  \n",
    "  '''\n",
    "  X = np.load(\"long_in.npy\")\n",
    "  y = np.load(\"long_out.npy\")\n",
    "  '''\n",
    "  '''\n",
    "  X = np.load(\"single_std_in.npy\")\n",
    "  y = np.load(\"single_std_out.npy\")\n",
    "  \n",
    "  X = np.load(\"rand_st_in.npy\")\n",
    "  y = np.load(\"rand_st_out.npy\")\n",
    "  '''\n",
    "  \n",
    "  np.random.seed(0)\n",
    "  np.random.shuffle(y)\n",
    "  np.random.seed(0)\n",
    "  np.random.shuffle(X)\n",
    "\n",
    "  # TODO: Split the data into training, validation and test sets\n",
    "  length=len(y)\n",
    "  num_training=int(length*training_ratio)\n",
    "  num_val = int(length*val_ratio)\n",
    "  num_test = min((length-num_training-num_val), int(length*test_ratio))\n",
    "  mask = range(num_training-1)\n",
    "  X_train = X[mask]\n",
    "  y_train = y[mask]\n",
    "  mask = range(num_training, num_training+num_test)\n",
    "  X_test = X[mask]\n",
    "  y_test = y[mask]\n",
    "  mask = range(num_training+num_test, num_training+num_test+num_val)\n",
    "  X_val = X[mask]\n",
    "  y_val = y[mask]\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "cwd = os.getcwd()\n",
    "datafile = cwd  + '/metu/dataset/Part_1.mat'\n",
    "#datafile = 'metu/dataset/part1_dataset.mat' #TODO: PATH to your data file\n",
    "input_size = 1000 # TODO: Size of the input of the network\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data(datafile)\n",
    "print \"Number of instances in the training set: \", len(X_train)\n",
    "print \"Number of instances in the validation set: \", len(X_val)\n",
    "print \"Number of instances in the testing set: \", len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting on a tiny set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2500: loss 24812.250074\n",
      "iteration 100 / 2500: loss 21871.663668\n",
      "iteration 200 / 2500: loss 18935.398406\n",
      "iteration 300 / 2500: loss 26052.586229\n",
      "iteration 400 / 2500: loss 33130.058803\n",
      "iteration 500 / 2500: loss 35710.504353\n",
      "iteration 600 / 2500: loss 28327.942187\n",
      "iteration 700 / 2500: loss 32596.790524\n",
      "iteration 800 / 2500: loss 30730.188100\n",
      "iteration 900 / 2500: loss 26148.797062\n",
      "iteration 1000 / 2500: loss 23278.686058\n",
      "iteration 1100 / 2500: loss 27517.681473\n",
      "iteration 1200 / 2500: loss 31730.059124\n",
      "iteration 1300 / 2500: loss 27199.790824\n",
      "iteration 1400 / 2500: loss 29716.269166\n",
      "iteration 1500 / 2500: loss 17197.890625\n",
      "iteration 1600 / 2500: loss 5538.944007\n",
      "iteration 1700 / 2500: loss 7170.788001\n",
      "iteration 1800 / 2500: loss 814.704278\n",
      "iteration 1900 / 2500: loss 22.880757\n",
      "iteration 2000 / 2500: loss 38.002809\n",
      "iteration 2100 / 2500: loss 25.853388\n",
      "iteration 2200 / 2500: loss 18.149809\n",
      "iteration 2300 / 2500: loss 40.904985\n",
      "iteration 2400 / 2500: loss 31.229456\n",
      "\n",
      "train error:  21.078067330570956\n",
      "\n",
      "[[110.32438743  78.98962124]\n",
      " [ 88.79400243  70.13176046]\n",
      " [138.13523925  91.46710422]]\n",
      "[[110.51663326  78.71646301]\n",
      " [ 92.04246003  65.56403408]\n",
      " [134.87374363  96.05716667]]\n"
     ]
    }
   ],
   "source": [
    "from cs231n.classifiers.three_layer_net_for_regression import ThreeLayerNet\n",
    "\n",
    "tiny_fitter = ThreeLayerNet(input_size, [400,100], 2)\n",
    "\n",
    "\n",
    "# Train the network\n",
    "stats = tiny_fitter.train(X_train[:3], y_train[:3], X_train[:3], y_train[:3],\n",
    "            num_iters=2500, batch_size=3, dropout=1.0, momentum=0.9,\n",
    "            learning_rate=1e-6, learning_rate_decay=1.0,\n",
    "            reg=0.0, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "#val_err = ... # TODO: Perform prediction on the validation set\n",
    "y_pred_train = tiny_fitter.predict(X_train[:3])\n",
    "train_err = np.sum(np.square(y_pred_train - y_train[:3]), axis=1).mean()\n",
    "\n",
    "print\n",
    "print 'train error: ', train_err\n",
    "print\n",
    "\n",
    "print y_train[:3]\n",
    "print y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now train our network on the PPG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 15000: loss 1212843.149191\n",
      "iteration 100 / 15000: loss 965337.149450\n",
      "iteration 200 / 15000: loss 737560.241382\n",
      "iteration 300 / 15000: loss 616308.575468\n",
      "iteration 400 / 15000: loss 489871.621735\n",
      "iteration 500 / 15000: loss 394037.408788\n",
      "iteration 600 / 15000: loss 283531.736740\n",
      "iteration 700 / 15000: loss 223919.277927\n",
      "iteration 800 / 15000: loss 187610.349296\n",
      "iteration 900 / 15000: loss 168051.590669\n",
      "iteration 1000 / 15000: loss 121951.975180\n",
      "iteration 1100 / 15000: loss 108500.967042\n",
      "iteration 1200 / 15000: loss 92259.021854\n",
      "iteration 1300 / 15000: loss 67599.021720\n",
      "iteration 1400 / 15000: loss 62086.911528\n",
      "iteration 1500 / 15000: loss 60027.743975\n",
      "iteration 1600 / 15000: loss 48387.978161\n",
      "iteration 1700 / 15000: loss 42487.272690\n",
      "iteration 1800 / 15000: loss 64803.094544\n",
      "iteration 1900 / 15000: loss 43204.780335\n",
      "iteration 2000 / 15000: loss 41289.677119\n",
      "iteration 2100 / 15000: loss 42019.240178\n",
      "iteration 2200 / 15000: loss 39803.568316\n",
      "iteration 2300 / 15000: loss 37108.979297\n",
      "iteration 2400 / 15000: loss 43435.937308\n",
      "iteration 2500 / 15000: loss 38316.623439\n",
      "iteration 2600 / 15000: loss 31586.464260\n",
      "iteration 2700 / 15000: loss 30419.821774\n",
      "iteration 2800 / 15000: loss 35226.217829\n",
      "iteration 2900 / 15000: loss 43773.171786\n",
      "iteration 3000 / 15000: loss 40954.830947\n",
      "iteration 3100 / 15000: loss 35013.271227\n",
      "iteration 3200 / 15000: loss 36980.291572\n",
      "iteration 3300 / 15000: loss 36567.980054\n",
      "iteration 3400 / 15000: loss 43615.119777\n",
      "iteration 3500 / 15000: loss 33207.321318\n",
      "iteration 3600 / 15000: loss 36640.371038\n",
      "iteration 3700 / 15000: loss 32251.957142\n",
      "iteration 3800 / 15000: loss 45531.921166\n",
      "iteration 3900 / 15000: loss 34641.817541\n",
      "iteration 4000 / 15000: loss 42211.399227\n",
      "iteration 4100 / 15000: loss 40417.850885\n",
      "iteration 4200 / 15000: loss 39584.811684\n",
      "iteration 4300 / 15000: loss 32128.856532\n",
      "iteration 4400 / 15000: loss 36327.925202\n",
      "iteration 4500 / 15000: loss 42678.734543\n",
      "iteration 4600 / 15000: loss 30936.854630\n",
      "iteration 4700 / 15000: loss 39650.595577\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-0a1368ed0841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             reg=0.001, verbose=True)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Predict on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismail/deeplearning/HW1_for_students/cs231n/classifiers/three_layer_net_for_regression.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, learning_rate, learning_rate_decay, reg, dropout, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Check error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mval_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mtrain_err_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mval_err_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismail/deeplearning/HW1_for_students/cs231n/classifiers/three_layer_net_for_regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# TODO: Implement this function; it should be VERY simple!                #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m###########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;31m###########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m#                              END OF YOUR CODE                           #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismail/deeplearning/HW1_for_students/cs231n/classifiers/three_layer_net_for_regression.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y, reg, dropout)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# shape (N, C).                                                             #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mhidden1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mrelu1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrelu1_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m/\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now, let's train a neural network\n",
    "# 3 Layer net\n",
    "from cs231n.classifiers.three_layer_net_for_regression import ThreeLayerNet\n",
    "\n",
    "input_size = input_size\n",
    "hidden_size = 700\n",
    "hidden_sizes = [700, 400] # TODO: Choose a suitable hidden layer size\n",
    "num_classes = 2 # We have two outputs\n",
    "\n",
    "net = ThreeLayerNet(input_size, hidden_sizes, num_classes)\n",
    "#net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=5000, batch_size=128, dropout=0.9,\n",
    "            learning_rate=1e-6, learning_rate_decay=1,\n",
    "            reg=0.001, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "#val_err = ... # TODO: Perform prediction on the validation set\n",
    "y_pred_train = net.predict(X_train)\n",
    "train_err = np.sum(np.square(y_pred_train - y_train), axis=1).mean()\n",
    "\n",
    "y_pred_val = net.predict(X_val)\n",
    "y_pred_test = net.predict(X_test)\n",
    "val_err = np.sum(np.square(y_pred_val - y_val), axis=1).mean()\n",
    "test_err = np.sum(np.square(y_pred_test - y_test), axis=1).mean()\n",
    "\n",
    "print\n",
    "print 'train/val/test error: ', train_err , val_err, test_err\n",
    "print\n",
    "\n",
    "idx = np.random.choice(y_val.shape[0], 5)\n",
    "print y_val[idx]\n",
    "print y_pred_val[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug the training and improve learning\n",
    "You should be able to get a validation error of 5.\n",
    "\n",
    "So far so good. But, is it really good? Let us plot the validation and training errors to see how good the network did. Did it memorize or generalize? Discuss your observations and conclusions. If its performance is not looking good, propose and test measures. This is the part that will show me how well you have digested everything covered in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation errors\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "train = plt.plot(stats['train_err_history'], label='train')\n",
    "val = plt.plot(stats['val_err_history'], label='val')\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.title('Classification error history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning and Improving Your Network (Bonus)\n",
    "There are many aspects and hyper-parameters you can play with. Do play with them and find the best setting here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs: 32 lr: 1e-05 rg: 0.5 dr: 0.5\n",
      "train/val err: 566.6332003759806 562.0242200682388\n",
      "\n",
      "bs: 32 lr: 1e-05 rg: 0.5 dr: 1.0\n",
      "train/val err: 566.5962689612354 562.2851977891905\n",
      "\n",
      "bs: 32 lr: 1e-05 rg: 0.001 dr: 0.5\n",
      "train/val err: 571.6366592864364 570.9782198810125\n",
      "\n",
      "bs: 32 lr: 1e-05 rg: 0.001 dr: 1.0\n",
      "train/val err: 566.6017455349102 562.3707172733087\n",
      "\n",
      "bs: 32 lr: 1e-05 rg: 0.0 dr: 0.5\n",
      "train/val err: 566.6852384305731 562.8515455774327\n",
      "\n",
      "bs: 32 lr: 1e-05 rg: 0.0 dr: 1.0\n",
      "train/val err: 567.8725726108629 562.2554808083893\n",
      "\n",
      "bs: 32 lr: 1e-06 rg: 0.5 dr: 0.5\n",
      "train/val err: 563.8613021983213 563.0019774258633\n",
      "\n",
      "bs: 32 lr: 1e-06 rg: 0.5 dr: 1.0\n",
      "train/val err: 551.3983697520571 543.7521959911569\n",
      "\n",
      "bs: 32 lr: 1e-06 rg: 0.001 dr: 0.5\n",
      "train/val err: 596.8335043303956 602.6473569731916\n",
      "\n",
      "bs: 32 lr: 1e-06 rg: 0.001 dr: 1.0\n",
      "train/val err: 551.1226276823005 543.5266325930113\n",
      "\n",
      "bs: 32 lr: 1e-06 rg: 0.0 dr: 0.5\n",
      "train/val err: 546.5618766307459 541.3965484768689\n",
      "\n",
      "bs: 32 lr: 1e-06 rg: 0.0 dr: 1.0\n",
      "train/val err: 617.7507076374998 629.9026336592285\n",
      "\n",
      "bs: 32 lr: 5e-07 rg: 0.5 dr: 0.5\n",
      "train/val err: 957.8639482661403 961.5631170785011\n",
      "\n",
      "bs: 32 lr: 5e-07 rg: 0.5 dr: 1.0\n",
      "train/val err: 670.4291046071263 663.0610037655969\n",
      "\n",
      "bs: 32 lr: 5e-07 rg: 0.001 dr: 0.5\n",
      "train/val err: 948.069722532218 940.5973835996753\n",
      "\n",
      "bs: 32 lr: 5e-07 rg: 0.001 dr: 1.0\n",
      "train/val err: 647.4446508998823 643.2788928899109\n",
      "\n",
      "bs: 32 lr: 5e-07 rg: 0.0 dr: 0.5\n",
      "train/val err: 953.9185670152302 948.0128615581756\n",
      "\n",
      "bs: 32 lr: 5e-07 rg: 0.0 dr: 1.0\n",
      "train/val err: 786.4372713846383 782.1555628336133\n",
      "\n",
      "bs: 32 lr: 1e-07 rg: 0.5 dr: 0.5\n",
      "train/val err: 1307.041100925937 1302.867377401094\n",
      "\n",
      "bs: 32 lr: 1e-07 rg: 0.5 dr: 1.0\n",
      "train/val err: 1332.2370728978103 1331.2320337705864\n",
      "\n",
      "bs: 32 lr: 1e-07 rg: 0.001 dr: 0.5\n",
      "train/val err: 1312.1191192797096 1311.0407821193076\n",
      "\n",
      "bs: 32 lr: 1e-07 rg: 0.001 dr: 1.0\n",
      "train/val err: 1332.650575241719 1331.107553718516\n",
      "\n",
      "bs: 32 lr: 1e-07 rg: 0.0 dr: 0.5\n",
      "train/val err: 1317.2064863533392 1317.707549666935\n",
      "\n",
      "bs: 32 lr: 1e-07 rg: 0.0 dr: 1.0\n",
      "train/val err: 1343.9086486633978 1346.0491693737836\n",
      "\n",
      "bs: 64 lr: 1e-05 rg: 0.5 dr: 0.5\n",
      "train/val err: 568.1267591137105 565.7081080118354\n",
      "\n",
      "bs: 64 lr: 1e-05 rg: 0.5 dr: 1.0\n",
      "train/val err: 435990417718588.6 435990451422695.9\n",
      "\n",
      "bs: 64 lr: 1e-05 rg: 0.001 dr: 0.5\n",
      "train/val err: 566.9825108815106 563.5727510212113\n",
      "\n",
      "bs: 64 lr: 1e-05 rg: 0.001 dr: 1.0\n",
      "train/val err: 566.5994064743727 562.3199764831035\n",
      "\n",
      "bs: 64 lr: 1e-05 rg: 0.0 dr: 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-b387ec1065f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mnum_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     reg=regularization_strengths[j])\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mval_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mtrain_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismail/deeplearning/HW1_for_students/cs231n/classifiers/three_layer_net_for_regression.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, learning_rate, learning_rate_decay, reg, dropout, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;31m# Compute loss and gradients using the current minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m       \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ismail/deeplearning/HW1_for_students/cs231n/classifiers/three_layer_net_for_regression.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y, reg, dropout)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mhidden1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mrelu1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrelu1_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m/\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mrelu1_out\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_net = None\n",
    "\n",
    "results = {}\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "batch_sizes = [32, 64, 128]\n",
    "dropout_probs = [0.5, 1.0]\n",
    "learning_rates = [1e-5,1e-6, 5e-7, 1e-7]\n",
    "regularization_strengths = [5e-1,1e-3, 0.0]\n",
    "k=0\n",
    "while k<len(batch_sizes):\n",
    "    i=0\n",
    "    while i< len(learning_rates):\n",
    "        j = 0\n",
    "        while j < len(regularization_strengths):    \n",
    "            d = 0\n",
    "            while d < len(dropout_probs):\n",
    "                print 'bs:',batch_sizes[k], \"lr:\", learning_rates[i], \"rg:\", regularization_strengths[j], 'dr:', dropout_probs[d]\n",
    "                \n",
    "                net = ThreeLayerNet(input_size, [500,300], num_classes)\n",
    "                stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                    num_iters=3000, batch_size=batch_sizes[k], dropout=dropout_probs[d],\n",
    "                    learning_rate=learning_rates[i], learning_rate_decay=0.95,\n",
    "                    reg=regularization_strengths[j])\n",
    "                val_err = np.sum(np.square(net.predict(X_val) - y_val), axis=1).mean()\n",
    "                train_err = np.sum(np.square(net.predict(X_train) - y_train), axis=1).mean()\n",
    "                \n",
    "                print 'train/val err:',train_err, val_err\n",
    "                if best_val > val_err:\n",
    "                    best_val = val_err\n",
    "                    best_net = net\n",
    "                results[(batch_sizes[k],learning_rates[i], regularization_strengths[j], dropout_probs[d])] = (train_err, val_err)\n",
    "                print\n",
    "                d=d+1\n",
    "            j=j+1\n",
    "        i=i+1\n",
    "    k=k+1\n",
    "\n",
    "print 'best validation error achieved during cross-validation: %f' % best_val\n",
    "\n",
    "test_error = np.sum(np.square(best_net.predict(X_test) - y_test), axis=1).mean()\n",
    "print 'Test error: ', test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried several architectures with 2 and 3 layers with different learning rates, regularizations strengths, dropout rates etc. I was always stuck in a local minima. I have also tried to collect to data with different strides and randomizations. Those did not help as well. I could not even achieved 0.0 loss in tiny subsets of the data with sizes of 3, 5, 10, and 20. \n",
    "\n",
    "Only way I can tell it works is I could achieve 0.0 loss with 2 data points. Other than that no mentionable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
